\input{header.tex}

\begin{document}
\section{Background}
The problem being solved is the Poisson problem in one dimension:
$$ -\grad^2\vec{u} = f $$
$$ f(\vec{x}) = \vec{x} $$
Defined on the following domain with Dirichlet boundary conditions:
$$ \Omega = \left[-1, 1\right]$$
$$ \partial\Omega = 0 $$
This is discretized using finite differences on a grid of $N=31$ equispaced points and results in the following system:
$$ \mat{A}\vec{u} = \vec{x} $$
where
$$\mat{A} = \frac{1}{h^2} \text{tridiag}\left(\left[-1, 2, -1\right]\right)$$
$$h = \frac{1}{N+1}$$

The above linear system is solved using a two-level V cycle multigrid method using the following steps:

\begin{enumerate}
\item Generate an initial random guess, $\vec{u}_0$.
\item Pre-smooth the guess using $\nu=5$ iterations of Jacobi using weight $\omega$.
\item Restrict the grid and residual using the operator $\mat{R}=\mat{P}^T$.  The given interpolation operator $\mat{P}$ is constructed as the \textit{ideal interpolation operator}.
\item Perform a linear solve on the restricted residual to obtain the coarse solution.
\item Interpolate the solution to the fine grid using operator $\mat{P}$.
\item Post-smooth using 5 iterations of Jacobi, with same weight $\omega$.
\end{enumerate}

The Jacobi smoothing weight that is used, $\omega$, is the optimal weight that minimizes the overall factor of convergence for the method:

$$\omega = \arg \min_{\omega\in\left(0,1\right)} \lim_{k\to\infty} \frac{ \vec{u}_{k+1} - \vec{u}^* }{ \vec{u}_{k} - \vec{u}^* }$$

where $\vec{u}^*$ is the ``optimal'' solution, precomputed by a dense linear solve.  Computationally, $\omega$ is approximated using a brute-force sweep of values in $\left(0, 1\right) $.

As a side note, brief experimentation suggests the Jacobi weight as a function of convergence factor to be unimodal with the 1D Poisson problem (Fig \ref{fig:jacobi_unimodal}).  This brute-force sweep could perhaps be converted to a numerical optimization to speed up running time.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.7]{figures/10-16-20-jacobi-conv-unimodality.png}
  \caption{Graph showing the factor of convergence vs Jacobi weight for a grid with coarsening by 3}
  \label{fig:jacobi_unimodal}
\end{figure}

\section{Generating Coarse Grids}

To train the model, a set of 6016 C/F grids were randomly generated.  A ``reference'' C/F grid was first generated such that every third grid point is a coarse point, and the rest are fine points.  Educated readers may recognize this as a \textit{coarsening by 3}.

This reference grid was randomly permuted such that each grid point had a random probability of being flipped to the opposite value.  I.e. coarse point to fine, and fine point to coarse.  Random trials of the following probabilities were used:

$$ p = \left\{0.01, \quad 0.05, \quad 0.1, \quad 0.25, \quad 0.5, \quad 0.75\right\} $$

For each value of $p$, 1000 random grids were generated according to the above permutation strategy.  Each random grid was then used to solve the defined Poisson problem, and the optimal Jacobi weight, $\omega$, and convergence factor were recorded.

Then, in an attempt to train the model on a few ``sane'' grids, 16 grids with uniform spacing between coarse/fine points were generated.  The coarsening factors used were:

$$r = \left\{
\frac{1}{9},\quad
\frac{1}{8},\quad
\frac{1}{7},\quad
\frac{1}{6},\quad
\frac{1}{5},\quad
\frac{1}{4},\quad
\frac{1}{3},\quad
\frac{1}{2},\quad
2,\quad
3,\quad
4\quad
5,\quad
6,\quad
7,\quad
8,\quad
9
\right\}$$

where values of $r<1$ refer to grids where every $\frac{1}{r}$ points is a fine point and the rest coarse, and $r>1$ refers to grids with every $r$ points being a coarse point and the rest fine.  These grids were then also used to solve the Poisson problem and their weights and convergence factors recorded.

The code used to generate these grids is given in \texttt{grids/gen\_grids.py}.  Note that it can take a decent amount of time to run, approximately 21 minutes on my machine to completely finish.

\section{CNN Model}

In an attempt to predict optimal Jacobi weight given an arbitrary grid a CNN of the following architecture was trained\footnote{I neither claim that this CNN is optimal nor was its architecture particularly well thought-out.  I am sure that tweaking the layers will obtain better results}:

\begin{enumerate}
\item 1-dimensional convolutional layer with kernel size $5$.  Input 1 layer, output 2 layers.  ReLU activation function.
\item 1-dimensional convolutional layer with kernel size $5$.  Input 2 layers, output 8 layers.  ReLU activation function.
%\item Max pooling layer with kernel size $2$, stride $2$.  This takes the maximum element of each block of $2$ elements, and effectively cuts the size of the input tensor in half.
\item Fully connected layer with input size 184, output size 92.  ReLU activation function.
\item Fully connected layer with input size 92, output size 1.  ReLU activation function.
\end{enumerate}

\subsection{Training}

The previously generated set of 6016 random grids was split into training and testing sets, with a random $85\%$ of entries going to the training set and the remaining $15\%$ going into the testing set.

The grids were formed into a $N\times 1 \times n$ tensor, with $N=6016$ being the number of grids and $n=31$ being the grid size.  The tensor representation was formed by means of assigning $1$ to coarse points and $-1$ to fine points:

$$
T_{i1k} = \begin{cases}
1 & \text{point $k$ in grid $i$ is coarse}\\
-1 & \text{point $k$ in grid $i$ is fine}
\end{cases}
$$

Of course, this assignment is arbitrary and swapping the values would produce similar results.  The input weights were then normalized such that their range lies between $\left[0, 1\right]$.  When displaying any output from the CNN, this transformation must be undone for the results to be sensible.  The grid dataset class contains a \texttt{scale\_output()} method to do this in \texttt{jacobi-cnn/model.py}.

This CNN was run through 200 iterations of a training loop using a MSE (mean-square error) loss function.  In each iteration, minibatches of size $500$ were used to train and backpropagate the model.  Since the number of training samples is not evenly divisible by $500$, these ``extra'' samples were arbitrarily discarded at each iteration.  The MSE loss and L1 loss at each iteration is given by Figures \ref{fig:loss_log}, \ref{fig:loss_linear}.

\subsection{Results}

Results of plotting the true vs predicted Jacobi weights can be seen in Figures \ref{fig:test} (Testing dataset) and \ref{fig:train} (Training dataset).  For true weights $>0.7$ the CNN tends to under-predict the weight, while $<0.7$ it tends to over-predict.  An interesting thing to note is the horizontal line of predictions at $y\approx0.67$; this could indicate that the CNN was train for too few iterations.  It is also interesting that there are no predicted values under this horizontal line, perhaps more training samples are needed.


\begin{figure}[h]
  \centering
  \includegraphics[scale=0.7]{figures/10-16-20-mse-loss.png}
  \includegraphics[scale=0.7]{figures/10-16-20-l1-loss.png}
  \caption{MSE, L1 loss per training iteration}
  \label{fig:loss_log}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.7]{figures/10-16-20-mse-linear.png}
  \includegraphics[scale=0.7]{figures/10-16-20-l1-linear.png}
  \caption{MSE, L1 loss per training iteration.  Linear $y$-axis, in the event that someone complained about the log scaling.}
  \label{fig:loss_linear}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.7]{figures/10-16-20-jacobi-test-scaled.png}
  \caption{Predicted vs Actual Jacobi Weights (Testing Dataset)}
  \label{fig:test}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.7]{figures/10-16-20-jacobi-train-scaled.png}
  \caption{Predicted vs Actual Jacobi Weights (Training Dataset)}
  \label{fig:train}
\end{figure}

\end{document}
